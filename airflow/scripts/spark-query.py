import sys
from pyspark.sql import SparkSession

run_date = sys.argv[1]  # â† ngÃ y truyá»n tá»« Airflow CLI
print(f"ğŸ“… Received run_date: {run_date}")

spark = SparkSession.builder \
    .appName("QueryTransactionByDate") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# Parse run_date náº¿u cáº§n
from datetime import datetime
dt = datetime.strptime(run_date, "%Y-%m-%d")
year, month, day = dt.year, dt.month, dt.day

# Load dá»¯ liá»‡u phÃ¢n vÃ¹ng theo ngÃ y
df = spark.read.parquet("hdfs://hadoop:9000/data/transactions/partitioned")
df_filtered = df.filter(
    (df["year"] == year) & (df["month"] == month) & (df["day"] == day)
)

df_filtered.show(5)
spark.stop()